# synonym_manager.py
from __future__ import annotations
from typing import Dict, List
from lz_mysql import MySQLPool


class LexiconManager:
    """
    è´Ÿè´£ï¼š
    - æœç´¢è¯åº“èµ„æºåŠ è½½ï¼ˆåŒä¹‰è¯ / åœç”¨è¯ / ä¸“æœ‰åè¯ï¼‰
    - ç»Ÿä¸€ä»¤ search pipeline åš normalizeã€filter
    - æ”¯æŒèµ„æ–™åº“ â†” æ–‡æœ¬ çš„åŒæ­¥å¯¼å‡º

    æ ¸å¿ƒæ€æƒ³ï¼šè¯åº“ç»´æŠ¤åœ¨æ–‡æœ¬ / DBï¼Œ runtime ç”¨å†…å­˜è¯»ä¸€æ¬¡å³å¯ã€‚
    """

    # === å†…å­˜ç¼“å­˜ ===
    _synonym_map: dict[str, str] = {}     # e.g. {"å°æ­£å¤ª": "æ­£å¤ª"}
    _stop_words: set[str] = set()         # e.g. {"è§†é¢‘", "èµ„æº"}
    _proper_nouns: set[str] = set()       # e.g. {"æ­£å¤ªä¸æ­£å¤ª"}

    _syn_loaded: bool = False
    _stop_loaded: bool = False
    _proper_loaded: bool = False

    # æ–°å¢ï¼šè§„èŒƒè¯ -> è¯¥ç»„æ‰€æœ‰åŒä¹‰è¯ï¼ˆå«è‡ªå·±ï¼‰
    _canonical_variants: dict[str, set[str]] = {}

    # ======= ç»Ÿä¸€å…¥å£ =======
    @classmethod
    def ensure_loaded(
        cls,
        synonym_path: str = "search_synonyms.txt",
        stopword_path: str = "search_stopwords.txt",
        proper_path: str = "search_proper_nouns.txt"
    ) -> None:
        """
        ç»Ÿä¸€å…¥å£ï¼šä¿è¯åŒä¹‰è¯ã€åœç”¨è¯ã€ä¸“æœ‰åè¯éƒ½è‡³å°‘åŠ è½½ä¸€æ¬¡ã€‚
        æ¨èåœ¨ connect() æˆ– search ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶æ‰§è¡Œã€‚
        """
        cls.load_synonyms_once(synonym_path)
        cls.load_stop_words_once(stopword_path)
        cls.load_proper_nouns_once(proper_path)

    # ======= è½½å…¥åŒä¹‰è¯ =======
    @classmethod
    def load_synonyms_once(cls, path: str) -> None:
        import os

        if cls._syn_loaded:
            return

        if not os.path.exists(path):
            print(f"[Lexicon] æœªæ‰¾åˆ°åŒä¹‰è¯æ–‡ä»¶ï¼š{path}ï¼Œè·³è¿‡åŠ è½½", flush=True)
            cls._synonym_map = {}
            cls._syn_loaded = True
            return

        mapping: dict[str, str] = {}
        try:
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith("#"):
                        continue
                    parts = line.split()
                    canonical = parts[0]
                    mapping[canonical] = canonical  # canonical è‡ªå

                    for token in parts[1:]:
                        mapping[token] = canonical

            cls._synonym_map = mapping
            # ğŸ”¹ å»ºç«‹ canonical -> variants åå‘è¡¨
            canonical_variants: dict[str, set[str]] = {}
            for variant, canonical in mapping.items():
                g = canonical_variants.setdefault(canonical, set())
                g.add(canonical)
                g.add(variant)
            cls._canonical_variants = canonical_variants
            cls._syn_loaded = True
            print(f"[Lexicon] loaded {len(mapping)} synonym entries from {path}", flush=True)

        except Exception as e:
            print(f"[Lexicon] åŒä¹‰è¯æ–‡ä»¶è¯»å–å¤±è´¥: {e}", flush=True)
            cls._synonym_map = {}
            cls._syn_loaded = True

    # ======= è½½å…¥åœç”¨è¯ =======
    @classmethod
    def load_stop_words_once(cls, path: str) -> None:
        import os

        if cls._stop_loaded:
            return

        if not os.path.exists(path):
            print(f"[Lexicon] æœªæ‰¾åˆ°åœç”¨è¯æ–‡ä»¶ï¼š{path}", flush=True)
            cls._stop_words = set()
            cls._stop_loaded = True
            return

        words: set[str] = set()
        try:
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith("#"):
                        continue
                    token = line.split()[0]
                    if token:
                        words.add(token)

            cls._stop_words = words
            cls._stop_loaded = True
            print(f"[Lexicon] loaded {len(words)} stop-words from {path}", flush=True)

        except Exception as e:
            print(f"[Lexicon] åœç”¨è¯æ–‡ä»¶è¯»å–å¤±è´¥: {e}", flush=True)
            cls._stop_words = set()
            cls._stop_loaded = True

    # ======= è½½å…¥ä¸“æœ‰åè¯ =======
    @classmethod
    def load_proper_nouns_once(cls, path: str) -> None:
        import os

        if cls._proper_loaded:
            return

        if not os.path.exists(path):
            print(f"[Lexicon] æœªæ‰¾åˆ°ä¸“æœ‰åè¯æ–‡ä»¶ï¼š{path}", flush=True)
            cls._proper_nouns = set()
            cls._proper_loaded = True
            return

        nouns: set[str] = set()
        try:
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith("#"):
                        continue
                    token = line.split()[0]
                    if token:
                        nouns.add(token)

            cls._proper_nouns = nouns
            cls._proper_loaded = True
            print(f"[Lexicon] loaded {len(nouns)} proper nouns from {path}", flush=True)

        except Exception as e:
            print(f"[Lexicon] ä¸“æœ‰åè¯æ–‡ä»¶è¯»å–å¤±è´¥: {e}", flush=True)
            cls._proper_nouns = set()
            cls._proper_loaded = True

    # ======= å¯¹å¤–å¤„ç†æ¥å£ =======
    @classmethod
    def normalize_tokens(cls, tokens: list[str]) -> list[str]:
        if not cls._syn_loaded:
            cls.load_synonyms_once("search_synonyms.txt")

        if not cls._synonym_map:
            return tokens

        return [cls._synonym_map.get(t, t) for t in tokens]

    @classmethod
    def filter_stop_words(cls, tokens: list[str]) -> list[str]:
        if not cls._stop_loaded:
            cls.load_stop_words_once("search_stopwords.txt")

        if not cls._stop_words:
            return tokens

        # è‹¥è¯å±äºä¸“æœ‰åè¯ï¼Œä¸è¿‡æ»¤
        return [t for t in tokens if t not in cls._stop_words or t in cls._proper_nouns]


    @classmethod
    def expand_token(cls, token: str) -> list[str]:
        """
        åŒä¹‰è¯å åŠ ï¼š
        - è‹¥ token æœ‰åŒä¹‰è¯ï¼Œåˆ™è¿”å› [è§„èŒƒè¯ + æ‰€æœ‰å˜ä½“ + åŸè¯]ï¼ˆå»é‡ï¼‰
        - è‹¥æ²¡æœ‰åŒä¹‰è¯ï¼Œåˆ™åªè¿”å› [token]
        """
        if not cls._syn_loaded:
            cls.load_synonyms_once("search_synonyms.txt")

        if not token:
            return []

        # æ²¡æœ‰åŒä¹‰è¯è¡¨æ—¶ï¼Œç›´æ¥è¿”å›è‡ªå·±
        if not cls._synonym_map:
            return [token]

        canonical = cls._synonym_map.get(token, token)
        variants = cls._canonical_variants.get(canonical)
        if not variants:
            return [token]

        # å†åŠ ä¸ŠåŸå§‹ tokenï¼Œä¸€èµ·å»é‡
        s = set(variants)
        s.add(token)
        return [t for t in s if t]

    @classmethod
    def expand_tokens(cls, tokens: list[str]) -> list[list[str]]:
        """
        æŠŠä¸€ä¸² token å˜æˆã€ŒåŒä¹‰è¯ç»„ã€åˆ—è¡¨ï¼š
        ["æ»‘é¼ ", "ä¹°"] -> [
            [...æ‰€æœ‰å’Œâ€œæ»‘é¼ â€åŒç»„çš„è¯...],
            [...æ‰€æœ‰å’Œâ€œä¹°â€åŒç»„çš„è¯...]
        ]
        """
        groups: list[list[str]] = []
        for t in tokens:
            t = t.strip()
            if not t:
                continue
            groups.append(cls.expand_token(t))
        return groups



    @classmethod
    def reload_synonyms_from_file(cls, path: str = "search_synonyms.txt") -> None:
        """
        å¼ºåˆ¶ä»æœ¬åœ°æ–‡ä»¶é‡è½½åŒä¹‰è¯æ˜ å°„ã€‚
        """
        cls._syn_loaded = False
        cls.load_synonyms_once(path)

    @classmethod
    def reload_stop_words_from_file(cls, path: str = "search_stopwords.txt") -> None:
        """
        å¼ºåˆ¶ä»æœ¬åœ°æ–‡ä»¶é‡è½½åœç”¨è¯ã€‚
        """
        cls._stop_loaded = False
        cls.load_stop_words_once(path)