# lz_db.py
import asyncpg
import asyncio
import os
from lz_config import POSTGRES_DSN
from lz_memory_cache import MemoryCache
from datetime import datetime
import lz_var


DEFAULT_MIN = int(os.getenv("POSTGRES_POOL_MIN", "1"))
DEFAULT_MAX = int(os.getenv("POSTGRES_POOL_MAX", "5"))
ACQUIRE_TIMEOUT = float(os.getenv("POSTGRES_ACQUIRE_TIMEOUT", "10"))
COMMAND_TIMEOUT = float(os.getenv("POSTGRES_COMMAND_TIMEOUT", "60"))
CONNECT_TIMEOUT = float(os.getenv("POSTGRES_CONNECT_TIMEOUT", "10"))  # Êñ∞Â¢û



class DB:
    def __init__(self):
        self.dsn = POSTGRES_DSN
        self.pool: asyncpg.Pool | None = None
        self.cache = MemoryCache()


    async def connect(self):
        if self.pool is not None:
            return
        # ÔºàÂèØÈÄâÔºâÈáçËØï 2-3 Ê¨°ÔºåÈÅøÂÖç‰∏¥Êó∂ÁΩëÁªúÊ≥¢Âä®
        retries = int(os.getenv("POSTGRES_CONNECT_RETRIES", "2"))
        last_exc = None
        for attempt in range(retries + 1):
            try:
                self.pool = await asyncpg.create_pool(
                    dsn=self.dsn,
                    min_size=DEFAULT_MIN,
                    max_size=DEFAULT_MAX,
                    max_inactive_connection_lifetime=300,
                    command_timeout=COMMAND_TIMEOUT,
                    timeout=CONNECT_TIMEOUT,            # Êñ∞Â¢ûÔºöËøûÊé•Á∫ßË∂ÖÊó∂
                    statement_cache_size=1024,          # Á®≥ÊÄÅ‰ºòÂåñ
                )
                # È¢ÑÁÉ≠ÔºöËÆæÁΩÆÊó∂Âå∫/Â∫îÁî®Âêç
                async with self.pool.acquire(timeout=ACQUIRE_TIMEOUT) as conn:
                    await conn.execute("SET SESSION TIME ZONE 'UTC'")
                    await conn.execute("SET application_name = 'lz_app'")
                print("‚úÖ PostgreSQL ËøûÊé•Ê±†ÂàùÂßãÂåñÂÆåÊàê")
                return
            except Exception as e:
                last_exc = e
                if attempt < retries:
                    await asyncio.sleep(1.0 * (attempt + 1))
                else:
                    raise

    async def connect_bk(self):
        """ÂπÇÁ≠âËøûÊé• + Â∞èËøûÊé•Ê±†ÔºåÈÅøÂÖç TooManyConnections„ÄÇ"""
        if self.pool is not None:
            return
        self.pool = await asyncpg.create_pool(
            dsn=self.dsn,
            min_size=DEFAULT_MIN,
            max_size=DEFAULT_MAX,
            max_inactive_connection_lifetime=300,
            command_timeout=COMMAND_TIMEOUT,
        )
        # ÂèØÈÄâÔºöÈ¢ÑÁÉ≠‰∏ÄÊ¨°ËøûÊé•ÔºåËÆæÁΩÆÊó∂Âå∫/Â∫îÁî®Âêç
        async with self.pool.acquire(timeout=ACQUIRE_TIMEOUT) as conn:
            await conn.execute("SET SESSION TIME ZONE 'UTC'")
            await conn.execute("SET application_name = 'lz_app'")
        print("‚úÖ PostgreSQL ËøûÊé•Ê±†ÂàùÂßãÂåñÂÆåÊàê")

    async def disconnect(self):
        """‰ºòÈõÖÊñ≠Á∫øÔºåÁªô‰∏ªÁ®ãÂ∫è shutdown/finally Ë∞ÉÁî®„ÄÇ"""
        pool, self.pool = self.pool, None
        if pool is not None:
            await pool.close()

    def _normalize_query(self, keyword_str: str) -> str:
        return " ".join(keyword_str.strip().lower().split())

    async def _ensure_pool(self):
        if self.pool is None:
            raise RuntimeError("PostgreSQL pool is not connected. Call db.connect() first.")



    async def search_keyword_page_highlighted(self, keyword_str: str, last_id: int = 0, limit: int = 10):
        query = self._normalize_query(keyword_str)
        cache_key = f"highlighted:{query}:{last_id}:{limit}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached

        async with self.pool.acquire() as conn:
            rows = await conn.fetch(
                '''
                SELECT id, source_id, file_type,
                       ts_headline('simple', content, plainto_tsquery('simple', $1)) AS highlighted_content
                FROM sora_content
                WHERE content_seg_tsv @@ plainto_tsquery('simple', $1)
                  AND id > $2
                ORDER BY id ASC
                LIMIT $3
                ''',
                query, last_id, limit
            )
            result = [dict(r) for r in rows]
            self.cache.set(cache_key, result, ttl=60)  # ÁºìÂ≠ò 60 Áßí
            return result

    async def search_keyword_page_plain(self, keyword_str: str, last_id: int = 0, limit: int = None):
       
        query = self._normalize_query(keyword_str)

        # # ÂÖàÊãø keyword_id
        # keyword_id = await self.get_search_keyword_id(query)
        # redis_key = f"sora_search:{keyword_id}" if keyword_id else None

        # # Âè™Êúâ page 0 ÊâçÊü• redis
        # if redis_key and last_id == 0:
        #     cached_result = await lz_var.redis_manager.get_json(redis_key)
        #     if cached_result:
        #         return cached_result

        cache_key = f"plain:{query}:{last_id}:{limit}"
        cached = self.cache.get(cache_key)
        if cached:
            print(f"üîπ MemoryCache hit for {cache_key}")
            return cached

        # Êü•ËØ¢ pg
        async with self.pool.acquire() as conn:
            rows = await conn.fetch(
                '''
                SELECT id, source_id, file_type, content 
                FROM sora_content
                WHERE content_seg_tsv @@ plainto_tsquery('simple', $1)
                AND id > $2
                ORDER BY id DESC
                LIMIT $3
                ''',
                query, last_id, limit
            )
            result = [dict(r) for r in rows]

            # # Âè™Êúâ page 0 Â≠ò redis
            # if redis_key and last_id == 0 and result:
            #     await lz_var.redis_manager.set_json(redis_key, result, ttl=300)


            # Â≠ò MemoryCacheÔºåttl ÂèØ‰ª•Ë∞É 60 Áßí / 300 Áßí
            self.cache.set(cache_key, result, ttl=300)
            print(f"üîπ MemoryCache set for {cache_key}, {len(result)} items")

            return result

    async def upsert_file_extension(self,
        file_type: str,
        file_unique_id: str,
        file_id: str,
        bot: str,
        user_id: str = None
    ):
        now = datetime.utcnow()

        sql = """
            INSERT INTO file_extension (
                file_type, file_unique_id, file_id, bot, user_id, create_time
            ) VALUES ($1, $2, $3, $4, $5, $6)
            ON CONFLICT (file_unique_id, bot)
            DO UPDATE SET
                file_id = EXCLUDED.file_id,
                create_time = EXCLUDED.create_time
            
        """

        # print(f"Executing SQL:\n{sql.strip()}")
        # print(f"With params: {file_type}, {file_unique_id}, {file_id}, {bot}, {user_id}, {now}")
        await self._ensure_pool()
        async with self.pool.acquire() as conn:
            result = await conn.fetchrow(sql, file_type, file_unique_id, file_id, bot, user_id, now)



        # print("DB result:", dict(result) if result else "No rows returned")


        sql2 =  """
                SELECT id FROM sora_content WHERE source_id = $1 OR thumb_file_unique_id = $2
                """

        # print(f"Executing SQL:\n{sql.strip()}")
        # print(f"With params: {file_type}, {file_unique_id}, {file_id}, {bot}, {user_id}, {now}")

        async with self.pool.acquire(timeout=ACQUIRE_TIMEOUT) as conn:
            row = await conn.fetchrow(sql2, file_unique_id, file_unique_id)
            if row:
                content_id = row["id"]
                cache_key = f"sora_content_id:{content_id}"
                # ËøôÈáåÂéüÊú¨ÊòØ db.cache.delete(...)ÔºåÂ∫î‰∏∫ self.cache.delete(...)
                self.cache.delete(cache_key)

    #Ë¶ÅÂíå ananbot_utils.py ‰ΩúÊï¥Âêà
    async def search_sora_content_by_id(self, content_id: int):
        cache_key = f"sora_content_id:{content_id}"
        # print(f"Searching sora_content by id {content_id} with cache key {cache_key}")
        cached = self.cache.get(cache_key)
        if cached:
            print(f"\r\n\r\n173:Cache hit for {cache_key}")
            return cached
        
        # print(f"\r\n\r\nCache miss for {cache_key}, querying database...")
    
        async with self.pool.acquire() as conn:
            row = await conn.fetchrow(
                '''
                SELECT s.id, s.source_id, s.file_type, s.content, s.file_size, s.duration, s.tag,
                    s.thumb_file_unique_id,
                    m.file_id AS m_file_id, m.thumb_file_id AS m_thumb_file_id,
                    p.price as fee, p.file_type as product_type, p.owner_user_id, p.purchase_condition
                FROM sora_content s
                LEFT JOIN sora_media m ON s.id = m.content_id AND m.source_bot_name = $2
                LEFT JOIN product p ON s.id = p.content_id
                WHERE s.id = $1
                ''',
                content_id, lz_var.bot_username
            )
            
            if not row:
                # print(f"\r\n\r\nNo sora_content found for id {content_id}"  )
                return None  # Êú™ÊâæÂà∞ÂÜÖÂÆπ
            # print(f"\r\n\r\nDatabase returned row for content_id {content_id}: {row}")

            row = dict(row)

            # ÂÖàÁî® m Ë°®ËµÑÊñô
            file_id = row.get("m_file_id")
            thumb_file_id = row.get("m_thumb_file_id")

            # Ëã•Áº∫ÂÖ∂‰∏≠‰ªª‰∏ÄÔºåÂàôÂ∞ùËØïÁî® file_extension Êü•Ë°•
            if not file_id or not thumb_file_id:

                # print(f"\r\n\r\n>>> Fetching file extensions for {row.get('source_id')} and {row.get('thumb_file_unique_id')}")

                extension_rows = await conn.fetch(
                    '''
                    SELECT file_unique_id, file_id
                    FROM file_extension
                    WHERE file_unique_id = ANY($1::text[])
                    AND bot = $2
                    ''',
                    [row.get("source_id"), row.get("thumb_file_unique_id")],
                    lz_var.bot_username
                )
                ext_map = {r["file_unique_id"]: r["file_id"] for r in extension_rows}

                # print(f"Fetched {len(extension_rows)} file extensions for content_id {content_id}")
                # Â¶ÇÊûú file_id ËøòÊ≤°ÂÄºÔºåÂ∞ùËØïÁî® source_id Âíå thumb_file_unique_id Êü•Êâæ
                # print(f"{ext_map}")
                # print(f"{row}")
               

                if not file_id and row.get("source_id") in ext_map:
                    file_id = ext_map[row["source_id"]]

                if not thumb_file_id and row.get("thumb_file_unique_id") in ext_map:
                    thumb_file_id = ext_map[row["thumb_file_unique_id"]]

            # Â¶ÇÊûú‰∏§‰∏™ÈÉΩÊúâÂÄºÔºåÂ∞± upsert ÂÜôÂÖ• sora_media
            if file_id and thumb_file_id:
                # print(f"\r\n\r\n>>>Upserting sora_media for content_id {content_id} with file_id {file_id} and thumb_file_id {thumb_file_id}")
                await conn.execute(
                    '''
                    INSERT INTO sora_media (content_id, file_id, thumb_file_id, source_bot_name)
                    VALUES ($1, $2, $3, $4)
                    ON CONFLICT (content_id, source_bot_name)
                    DO UPDATE SET
                        file_id = EXCLUDED.file_id,
                        thumb_file_id = EXCLUDED.thumb_file_id
                    ''',
                    content_id, file_id, thumb_file_id, lz_var.bot_username
                )

            result = {
                "id": row["id"],
                "source_id": row["source_id"],
                "file_type": row["file_type"],
                "content": row["content"],
                "file_size": row["file_size"],
                "duration": row["duration"],
                "tag": row["tag"],
                "file_id": file_id,
                "thumb_file_id": thumb_file_id,
                "thumb_file_unique_id": row.get("thumb_file_unique_id"),
                "fee": row.get("fee"),
                "product_type": row.get("product_type"),
                "owner_user_id": row.get("owner_user_id"),
                "purchase_condition": row.get("purchase_condition")
            }

            # print(f"\r\n\r\nFinal result for content_id {content_id}: {result}")

            # self.cache.set(cache_key, result, ttl=3600)
            self.cache.set(cache_key, result, ttl=3600)
            # print(f"Cache set for {cache_key}")
            return result

            # ËøîÂõû asyncpg Record Êàñ None

    async def get_next_content_id(self, current_id: int, offset: int) -> int | None:
        async with self.pool.acquire() as conn:
            if offset > 0:
                row = await conn.fetchrow(
                    """
                    SELECT id FROM sora_content
                    WHERE id > $1
                    ORDER BY id ASC
                    LIMIT 1
                    """,
                    current_id
                )
            else:
                row = await conn.fetchrow(
                    """
                    SELECT id FROM sora_content
                    WHERE id < $1
                    ORDER BY id DESC
                    LIMIT 1
                    """,
                    current_id
                )
            if row:
                return row["id"]
            return None

    async def get_file_id_by_file_unique_id(self, unique_ids: list[str]) -> list[str]:
        """
        Ê†πÊçÆÂ§ö‰∏™ file_unique_id ÂèñÂæóÂØπÂ∫îÁöÑ file_id ÂàóË°®„ÄÇ
        """
        if not unique_ids:
            return []

        async with self.pool.acquire() as conn:
            rows = await conn.fetch(
                '''
                SELECT file_id
                FROM file_extension
                WHERE file_unique_id = ANY($1::text[])
                AND bot = $2
                ''',
                unique_ids, lz_var.bot_username
            )
            # print(f"Fetched {len(rows)} rows for unique_ids: {unique_ids} {rows}")
            return [r['file_id'] for r in rows if r['file_id']]

    async def insert_search_log(self, user_id: int, keyword: str):
        async with self.pool.acquire() as conn:
            await conn.execute(
                """
                INSERT INTO search_log (user_id, keyword, search_time)
                VALUES ($1, $2, $3)
                """,
                user_id, keyword, datetime.utcnow()
            )

    async def upsert_search_keyword_stat(self, keyword: str):
        async with self.pool.acquire() as conn:
            result = await conn.execute(
                 """
                INSERT INTO search_keyword_stat (keyword, search_count, last_search_time)
                VALUES ($1, 1, CURRENT_TIMESTAMP)
                ON CONFLICT (keyword)
                DO UPDATE SET 
                    search_count = search_keyword_stat.search_count + 1,
                    last_search_time = CURRENT_TIMESTAMP
                """,
                keyword
            )
            return result

    async def get_search_keyword_id(self, keyword: str) -> int | None:
        async with self.pool.acquire() as conn:
            row = await conn.fetchrow(
                """
                WITH ins AS (
                    INSERT INTO search_keyword_stat (keyword)
                    VALUES ($1)
                    ON CONFLICT (keyword) DO NOTHING
                    RETURNING id
                )
                SELECT id FROM ins
                UNION ALL
                SELECT id FROM search_keyword_stat WHERE keyword = $1
                LIMIT 1
                """,
                keyword
            )
            return row["id"] if row else None

    async def get_keyword_by_id(self, keyword_id: int) -> str | None:
        
        cache_key = f"keyword:id:{keyword_id}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        
        async with self.pool.acquire() as conn:
            row = await conn.fetchrow(
                """
                SELECT keyword
                FROM search_keyword_stat
                WHERE id = $1
                """,
                keyword_id
            )
            if row:
                self.cache.set(cache_key, row["keyword"], ttl=300)
                return row["keyword"]
            return None


    async def get_latest_membership_expire(self, user_id: str | int) -> int | None:
        """
        Êü•ËØ¢ membership Ë°®‰∏≠ËØ• user_id ÁöÑÊúÄÊñ∞ÊúâÊïàÊúüÔºàexpire_timestamp ÊúÄÂ§ßÂÄºÔºâ„ÄÇ
        ËøîÂõûÂÄº:
          - int (UNIX Êó∂Èó¥Êà≥, Áßí)
          - None (Êú™ÊâæÂà∞ËÆ∞ÂΩï)
        """
        await self._ensure_pool()
        async with self.pool.acquire() as conn:
            row = await conn.fetchrow(
                """
                SELECT expire_timestamp
                FROM membership
                WHERE user_id = $1
                ORDER BY expire_timestamp DESC NULLS LAST
                LIMIT 1
                """,
                str(user_id)  # membership.user_id ÊòØ varchar
            )
            if row and row["expire_timestamp"] is not None:
                return int(row["expire_timestamp"])
            return None

    
    async def upsert_membership_bulk(self, rows: list[dict]) -> dict:
        """
        ÊâπÈáèÊää MySQL ÁöÑ membership Ë°åÂêåÊ≠•Âà∞ PostgreSQL„ÄÇ
        ÂÜ≤Á™ÅÈîÆÔºö (membership_id)  ‚Üê Âè™Êåâ membership_id ÂÜ≤Á™Å
        Êõ¥Êñ∞Á≠ñÁï•Ôºö
          - course_code, user_id     ‚Üê ‰ª• MySQL Ë°å‰∏∫ÂáÜÔºàEXCLUDEDÔºâ
          - create_timestamp         ‚Üê Âèñ LEAST(Áé∞Êúâ, Êñ∞ÂÄº)
          - expire_timestamp         ‚Üê Âèñ GREATEST(Áé∞Êúâ, Êñ∞ÂÄº)

        Ê≥®ÊÑèÔºö‰Ω†ÁöÑË°®Èáå‰ªçÊúâ unique_course_user Á∫¶ÊùüÔºå
        Â¶ÇÊûúÂêå‰∏ÄÁî®Êà∑Âêå‰∏ÄËØæÁ®ãÂ≠òÂú®Â§öÊù°‰∏çÂêå membership_id ÁöÑÂéÜÂè≤ËÆ∞ÂΩïÔºå
        ÂèØËÉΩ‰ºöËß¶ÂèëÂîØ‰∏ÄÂÜ≤Á™Å„ÄÇÈÄöÂ∏∏Âª∫ËÆÆ‰Ω†‰øùËØÅ‰∏Ä‰∫∫‰∏ÄËØæÂîØ‰∏Ä‰∏ÄÊù°ËÆ∞ÂΩïÔºõ
        Ëã•Á°ÆÈúÄÂ§öÊù°ÔºåËØ∑ËÄÉËôëÁßªÈô§ÊàñÊîπÈÄ†ËØ•ÂîØ‰∏ÄÁ¥¢Âºï„ÄÇ
        """
        if not rows:
            return {"ok": "1", "count": 0}

        await self._ensure_pool()
        sql = """
        INSERT INTO membership (membership_id, course_code, user_id, create_timestamp, expire_timestamp)
        VALUES ($1, $2, $3, $4, $5)
        ON CONFLICT (membership_id)
        DO UPDATE SET
            course_code      = EXCLUDED.course_code,
            user_id          = EXCLUDED.user_id,
            create_timestamp = LEAST(membership.create_timestamp, EXCLUDED.create_timestamp),
            expire_timestamp = GREATEST(membership.expire_timestamp, EXCLUDED.expire_timestamp)
        """
        try:
            async with self.pool.acquire() as conn:
                async with conn.transaction():
                    await conn.executemany(
                        sql,
                        [
                            (
                                int(r["membership_id"]),
                                str(r["course_code"]),
                                str(r["user_id"]),
                                int(r["create_timestamp"]),
                                int(r["expire_timestamp"]),
                            )
                            for r in rows
                        ],
                    )
            return {"ok": "1", "count": len(rows)}
        except Exception as e:
            return {"ok": "", "error": str(e)}


    
    async def get_album_list(self, content_id: int, bot_name: str) -> list[dict]:
        """
        Êü•ËØ¢Êüê‰∏™ album ‰∏ãÁöÑÊâÄÊúâÊàêÂëòÊñá‰ª∂ÔºàPostgreSQL ÁâàÔºâ
        - ÂØπÂ∫î PHP ÁöÑ get_album_list()
        - ‰ΩøÁî® asyncpgÔºåÂç†‰ΩçÁ¨¶ $1/$2
        - Ëã• m.file_id ‰∏∫Á©∫‰∏î‰ªé file_extension ÂåπÈÖçÂà∞ ext_file_idÔºåÂàôÂõûÂÜô/Êñ∞Â¢ûÂà∞ sora_media.file_id
        - ËøîÂõûÂÄºÔºölist[dict]
        """
        await self._ensure_pool()

        sql = """
            SELECT
                c.member_content_id,           -- Áî®‰∫éÂõûÂÜô sora_media.content_id
                s.source_id,
                c.file_type,
                s.content,
                s.file_size,
                s.duration,
                m.source_bot_name,
                m.thumb_file_id,
                m.file_id,
                fe.file_id AS ext_file_id
            FROM album_items AS c
            LEFT JOIN sora_content AS s
                ON c.member_content_id = s.id
            LEFT JOIN sora_media   AS m
                ON c.member_content_id = m.content_id
                AND m.source_bot_name   = $1
            LEFT JOIN file_extension AS fe
                ON fe.file_unique_id = s.source_id
                AND fe.bot            = $1
            WHERE c.content_id = $2
            ORDER BY c.file_type;
        """

        upsert_sql = """
            INSERT INTO sora_media (content_id, source_bot_name, file_id)
            VALUES ($1, $2, $3)
            ON CONFLICT (content_id, source_bot_name)
            DO UPDATE SET file_id = EXCLUDED.file_id
        """

        try:
            async with self.pool.acquire() as conn:
                rows = await conn.fetch(sql, bot_name, content_id)

                # ÂÖàÊääËÆ∞ÂΩïËΩ¨ÊàêÂèØÂèò dictÔºåÂπ∂Êî∂ÈõÜÈúÄË¶ÅÂõûÂÜôÁöÑÊù°ÁõÆ
                dict_rows: list[dict] = []
                to_upsert: list[tuple[int, str, str]] = []  # (content_id, bot_name, file_id)

                for rec in rows or []:
                    d = dict(rec)
                    # Áî® ext_file_id Â°´ÂÖÖËøîÂõûÂÄº
                    if d.get("file_id") is None and d.get("ext_file_id") is not None:
                        d["file_id"] = d["ext_file_id"]
                        # ÂáÜÂ§áÂõûÂÜô/Êñ∞Â¢ûÂà∞ sora_media
                        if d.get("member_content_id") is not None:
                            to_upsert.append((
                                int(d["member_content_id"]),
                                bot_name,
                                str(d["ext_file_id"]),
                            ))
                    dict_rows.append(d)

                # ÊâπÈáè UPSERT ÂõûÂÜô sora_mediaÔºàÂè™Â§ÑÁêÜÁ°ÆÂÆûÈúÄË¶ÅÂÜôÂÖ•ÁöÑÔºâ
                if to_upsert:
                    async with conn.transaction():
                        for cid, bn, fid in to_upsert:
                            await conn.execute(upsert_sql, cid, bn, fid)

                return dict_rows
        except Exception as e:
            print(f"‚ö†Ô∏è get_album_list Âá∫Èîô: {e}", flush=True)
            return []


   
    async def upsert_album_items_bulk(self, rows: list[dict]) -> int:
        """
        ÊâπÈáè UPSERT Âà∞ PostgreSQL ÁöÑ public.album_items
        ÂÜ≤Á™ÅÈîÆÔºö (content_id, member_content_id)
        Êõ¥Êñ∞Â≠óÊÆµÔºöfile_unique_id, file_type, position, updated_at, stage
        created_at ÈááÁî®Êó¢ÊúâÂÄºÔºà‰øùÊåÅÂéÜÂè≤ÔºâÔºåËã•ÂéüË°®‰∏∫Á©∫ÂàôÁî®ÈªòËÆ§ÂÄº
        ËøîÂõûÔºöÂèóÂΩ±ÂìçÔºàÊèíÂÖ•/Êõ¥Êñ∞ÔºâË°åÊï∞ÔºàËøë‰ººÔºâ
        """
        if not rows:
            return 0
        await self._ensure_pool()

        # Âè™Â∏¶ PG ÊúâÁöÑÂàóÔºåÊ≥®ÊÑè file_type Âú® PG ÊòØ textÔºàÂ∑≤Áî® CHECK Á∫¶ÊùüÔºâ
        payload = []
        for r in rows:
            payload.append((
                int(r["content_id"]),
                int(r["member_content_id"]),
                (r.get("file_unique_id") or None),
                (str(r.get("file_type") or "") or ""),  # ÂÖÅËÆ∏Á©∫Â≠óÁ¨¶‰∏≤
                int(r.get("position") or 0),
                str(r.get("stage") or "pending"),
            ))

        sql = """
            INSERT INTO album_items
                (content_id, member_content_id, file_unique_id, file_type, "position", stage, updated_at)
            VALUES ($1, $2, $3, $4, $5, $6, CURRENT_TIMESTAMP)
            ON CONFLICT (content_id, member_content_id)
            DO UPDATE SET
                file_unique_id = EXCLUDED.file_unique_id,
                file_type      = EXCLUDED.file_type,
                "position"     = EXCLUDED."position",
                stage          = EXCLUDED.stage,
                updated_at     = CURRENT_TIMESTAMP
        """
        affected = 0
        async with self.pool.acquire() as conn:
            async with conn.transaction():
                await conn.executemany(sql, payload)
                affected = len(payload)
        return affected


    async def delete_album_items_except(self, content_id: int, keep_member_ids: list[int]) -> int:
        """
        Âà†Èô§ PG ‰∏≠ËØ• content_id ‰∏ã„ÄÅ‰ΩÜ‰∏çÂú® keep_member_ids ÁöÑ album_items
        keep_member_ids ‰∏∫Á©∫Êó∂ÔºåÂà†Èô§ËØ• content_id ‰∏ãÊâÄÊúâËÆ∞ÂΩï
        ËøîÂõûÔºöÂà†Èô§Ë°åÊï∞
        """
        await self._ensure_pool()
        async with self.pool.acquire() as conn:
            if keep_member_ids:
                sql = """
                    DELETE FROM album_items
                    WHERE content_id = $1
                    AND member_content_id <> ALL($2::bigint[])
                """
                res = await conn.execute(sql, content_id, keep_member_ids)
            else:
                sql = "DELETE FROM album_items WHERE content_id = $1"
                res = await conn.execute(sql, content_id)
            # asyncpg ÁöÑËøîÂõûÁ±ª‰ºº 'DELETE 3'ÔºåÂèñÊï∞Â≠ó
            try:
                return int(res.split()[-1])
            except Exception:
                return 0


db = DB()